{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNMcL+JPow0sABUNhWYeD7j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavrick0/DEEP-LEARNING-EXPRIMENTS/blob/main/DL0606.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "sentences = [\n",
        "    \"I love machine learning\",\n",
        "    \"Natural language processing is fascinating\"\n",
        "]\n",
        "\n",
        "pos_tags = [\n",
        "    \"PRP VBP NN NN\",\n",
        "    \"NN NNS VBZ VBG\"\n",
        "]\n",
        "\n",
        "# Tokenize text and tags\n",
        "tokenizer_text = Tokenizer()\n",
        "tokenizer_text.fit_on_texts(sentences)\n",
        "text_sequences = tokenizer_text.texts_to_sequences(sentences)\n",
        "vocab_size_text = len(tokenizer_text.word_index) + 1\n",
        "\n",
        "tokenizer_tags = Tokenizer()\n",
        "tokenizer_tags.fit_on_texts(pos_tags)\n",
        "tag_sequences = tokenizer_tags.texts_to_sequences(pos_tags)\n",
        "vocab_size_tags = len(tokenizer_tags.word_index) + 1\n",
        "\n",
        "# Pad sequences\n",
        "max_len_text = max(len(seq) for seq in text_sequences)\n",
        "max_len_tags = max(len(seq) for seq in tag_sequences)\n",
        "\n",
        "X = pad_sequences(text_sequences, maxlen=max_len_text, padding='post')\n",
        "y = pad_sequences(tag_sequences, maxlen=max_len_tags, padding='post')\n",
        "\n",
        "# One-hot encode the output tags\n",
        "y = np.array([tf.keras.utils.to_categorical(seq, num_classes=vocab_size_tags) for seq in y])\n",
        "\n",
        "# Define model parameters\n",
        "embedding_dim = 50\n",
        "hidden_units = 64\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_len_text,))\n",
        "encoder_embedding = Embedding(input_dim=vocab_size_text, output_dim=embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "encoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [encoder_state_h, encoder_state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_len_tags, vocab_size_tags))\n",
        "decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ , _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size_tags, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Prepare decoder inputs (shifted targets)\n",
        "decoder_input_data = np.zeros_like(y)\n",
        "decoder_input_data[:, 1:, :] = y[:, :-1, :]\n",
        "decoder_input_data[:, 0, :] = np.zeros((len(sentences), vocab_size_tags))\n",
        "\n",
        "# Train the model\n",
        "model.fit([X, decoder_input_data], y, epochs=10, batch_size=2, validation_split=0.1)\n",
        "\n",
        "def predict_pos(sentence):\n",
        "    seq = tokenizer_text.texts_to_sequences([sentence])\n",
        "    seq = pad_sequences(seq, maxlen=max_len_text, padding='post')\n",
        "\n",
        "    # Initialize decoder input for prediction\n",
        "    decoder_input = np.zeros((1, max_len_tags, vocab_size_tags))\n",
        "\n",
        "    # Predict POS tags\n",
        "    prediction = model.predict([seq, decoder_input])\n",
        "\n",
        "    # Get the predicted tags\n",
        "    predicted_tags = np.argmax(prediction, axis=-1)\n",
        "\n",
        "    # Return the POS tags as a string\n",
        "    return ' '.join(tokenizer_tags.index_word.get(tag, '') for tag in predicted_tags[0])\n",
        "\n",
        "# Test with a new sentence\n",
        "new_sentence = \"I enjoy deep learning\"\n",
        "print(predict_pos(new_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9nKu7xHz6_",
        "outputId": "2af18e11-7b65-480c-cf60-3511ce0f344c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step - accuracy: 0.5000 - loss: 1.9307 - val_accuracy: 0.0000e+00 - val_loss: 1.9634\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 673ms/step - accuracy: 0.5000 - loss: 1.9188 - val_accuracy: 0.0000e+00 - val_loss: 1.9653\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.5000 - loss: 1.9067 - val_accuracy: 0.0000e+00 - val_loss: 1.9673\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.5000 - loss: 1.8945 - val_accuracy: 0.0000e+00 - val_loss: 1.9694\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.5000 - loss: 1.8820 - val_accuracy: 0.0000e+00 - val_loss: 1.9717\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 0.5000 - loss: 1.8692 - val_accuracy: 0.0000e+00 - val_loss: 1.9741\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.5000 - loss: 1.8559 - val_accuracy: 0.0000e+00 - val_loss: 1.9767\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.5000 - loss: 1.8421 - val_accuracy: 0.0000e+00 - val_loss: 1.9795\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - accuracy: 0.5000 - loss: 1.8275 - val_accuracy: 0.0000e+00 - val_loss: 1.9826\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.5000 - loss: 1.8122 - val_accuracy: 0.0000e+00 - val_loss: 1.9859\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 440ms/step\n",
            "nn nn nn nn\n"
          ]
        }
      ]
    }
  ]
}